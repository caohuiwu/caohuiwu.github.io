---
title: 《es》分词器
date: 2020-06-04
categories:
  - [es, 分词器]
---

    这是“es”系列的第三篇文章，主要介绍的是ES的分词器。

<style>
.my-code {
   color: green;
}
.orange {
   color: rgb(255, 53, 2)
}
.red {
   color: red
}
code {
   color: #6260ff;
}
</style>

# 一、ES

<code class="red">elasticsearch</code>一个开源的分布式搜索引擎，可以用来实现搜索、日志统计、分析、系统监控等功能.

<!-- more -->

# 二、分词器

## 2.1、Analysis 和 Analyzer
`Analysis`： 文本分析是把全文本转换一系列单词(`term/token`)的过程，也叫分词。`Analysis`是通过`Analyzer`来实现的。

当一个文档被索引时，每个Field都可能会创建一个倒排索引（Mapping可以设置不索引该Field）。

倒排索引的过程就是将文档通过`Analyzer`分成一个一个的Term,每一个Term都指向包含这个Term的文档集合。

当查询query时，Elasticsearch会根据搜索类型决定是否对query进行analyze，然后和倒排索引中的term进行相关性查询，匹配相应的文档。

## 2.2.、Analyzer组成
`分析器（analyzer）`都由三种构件块组成的：`character filters` ， `tokenizers` ， `token filters`。

1) `character filter` 字符过滤器
在一段文本进行分词之前，先进行预处理，比如说最常见的就是，过滤html标签（<span>hello<span> --> hello），& --> and（I&you --> I and you）

2) `tokenizers` 分词器
英文分词可以根据空格将单词分开,中文分词比较复杂,可以采用机器学习算法来分词。

3) `Token filters` Token过滤器
将切分的单词进行加工。大小写转换（例将“Quick”转为小写），去掉词（例如停用词像“a”、“and”、“the”等等），或者增加词（例如同义词像“jump”和“leap”）。

三者顺序：`Character Filters` ---> `Tokenizer` ---> `Token Filter`

三者个数：analyzer = CharFilters（0个或多个） + Tokenizer(恰好一个) + TokenFilters(0个或多个)

## 2.3、Elasticsearch的内置分词器
Standard Analyzer - 默认分词器，按词切分，小写处理

Simple Analyzer - 按照非字母切分(符号被过滤), 小写处理

Stop Analyzer - 小写处理，停用词过滤(the,a,is)

Whitespace Analyzer - 按照空格切分，不转小写

Keyword Analyzer - 不分词，直接将输入当作输出

Patter Analyzer - 正则表达式，默认\W+(非字符分割)

Language - 提供了30多种常见语言的分词器

Customer Analyzer 自定义分词器

## 2.4、创建索引时设置分词器
```
PUT new_index
{
   "settings": {
      "analysis": {
         "analyzer": {
            "std_folded": {//自定义分词器 std_folded
               "type": "custom", //类型：custom 自定义分词器
               "tokenizer": "standard", //标准分词器
               "filter": [ //token filter，词项过滤器
                  "lowercase", //将词项转换为小写
                  "asciifolding" //非ASCII字符转换为对应的ASCII字符（如 `é` → `e`）。
               ]
            }
         }
      }
   },
   //字段映射：定义了两个字段 ： title 和 content
   "mappings": {
      "properties": {
         //title字段
         "title": {
            "type": "text", #类型：text(全文搜索字段)
            "analyzer": "std_folded" # 使用自定义分词器
         },
         //content字段
         "content": {
            "type": "text", #类型：text(全文搜索字段)
            "analyzer": "whitespace" #空白分词器
         }
      }
   }
}
```

### 2.4.1、测试
可以使用_analyze API测试分词器的效果
```
POST /new_index/_analyze
{
  "analyzer": "std_folded",
  "text": "Café Elástico"
}
```
输出：
```JSON
{
  "tokens": [
    {
      "token": "cafe",
      "start_offset": 0,
      "end_offset": 4,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "elastico",
      "start_offset": 5,
      "end_offset": 13,
      "type": "<ALPHANUM>",
      "position": 1
    }
  ]
}
```
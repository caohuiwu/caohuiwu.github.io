---
title: socket创建过程
date: 2019-06-16 21:38:37
tags: tcp socket
---

# 一、socket创建过程
## 1、建立socket, sock的关系
创建完sock变量之后，便是初始化sock结构体，并建立sock与socket之间的引用关系；调
用链如下：
    
    net/Socket.c:sys_socket()
        ->sock_create()
        ->__sock_create()
        ->net /ipv4/Af_inet.c:inet_create()
        ->net/core/Sock.c:sock_init_data()：
该函数主要工作是：

    a. 初始化sock结构的缓冲区、队列等；
    b. 初始化sock结构的状态为TCP_CLOSE；
    c. 建立socket与sock结构的相互引用关系；
    
<!--more-->        

## 2、使用tcp协议初始化sock：
inet_create()函数最后，通过相应的协议来初始化sock结构：这里调用的是tcp_prot的init钩子函数net/ipv4/Tcp_ipv4.c:tcp_v4_init_sock()，它主要是对tcp_sock和inet_connection_sock进行一些初始化；

## 3、socket与文件系统关联：
创建好与socket相关的结构后，需要与文件系统关联，详见sock_map_fd()函数：
    
    1) 申请文件描述符，并分配file结构和目录项结构；
    2) 关联socket相关的文件操作函数表和目录项操作函数表；
    3) 将file->private_date指向socket；
> fd代表这文件系统

socket与文件系统关联后，以后便可以通过文件系统read/write对socket进行操作了；

## 4、服务端socket启动过程：90端口

    a. 内核创建socket（socket有接收队列+发送队列）
    b. 然后会申请创建一个对应的TCP类型的fd，此fd处于监听状态
    c. 最后建立socket与文件系统的关联
    -------------------------------------------------
    1、netstat -natp
        此时会创建90端口TCP进程，处于listen状态，内核分配资源：fd+接收队列+发送队列。
    2、lsof -p pid
        该进程对应的fd，也处于listen状态
    3、其他
        该listen进程会一直存在，用于接受其他连接


## 5、客户端连接

    客户端创建socket，与服务端创建过程一致。（创建socket和fd）
    1、此时服务端未accept()
        a. 三次握手后，服务端会创建一个新的socket（socket有缓存队列用于接收消息）
        b. 未accept()，此时不会创建fd与该socket建立关联。
        c. 若要读取缓存队列中的消息，需要创建新进程(fd)，与该socket建立映射
    
        1、netstat -natp
            经过三次握手后，服务端会新创建一个socket，但是未分配给进程使用；
        2、客户端发送消息
            此时服务端的socket对应的rec-q接收队列会受到客户端发送的内容。
        ----此时服务端创建的socket未指定到具体的fd.
    2、服务端accept
        创建fd，此时socket会指定到这个fd
    
    每个进程的进程控制块task_struct中都有一个files_struct结构体，它保存了进程所有打开的文件，以文件描述符fd为索引即可找到对应的file对象，file对象中也包含了文件当前位置的信息

> serverSocket会创建一个监听的socket，监听客户端的连接。会为每一个client再创建一个socket，该socket用于和客户端进行数据通信。（如果不做处理，这些socket对应的fd会放入同一个selector）
基于一个selector，如果一个线程去遍历该selector会遇到性能问题，所以netty的主从多线程模式，会将为client创建的socket添加到另外一个新的selector，再起新线程去处理改selector，
从而解决netty单reactor中遇到洪峰连接的性能问题。（即selector内的socket不仅仅有连接请求，还有数据读写等请求）


# 二、其他

    socket都需要fd去接收
    socket：是一个四元组（客户端IP   客户端port     服务端IP     服务端port），内核级，
    四元组唯一确定一个连接


# 三、SocketChannel创建过程
## 3.1、SocketChannel和Socket的区别
SocketChannel是对传统Java Socket API的改进，主要是支持了非阻塞的读写。同时改进了传统的单向流API, Channel同时支持读写(其实就是加了个中间层Buffer)。

### 1、socketChannel原理
```
// sun.nio.ch.SelectorProvider
public SocketChannel openSocketChannel() throws IOException {
    // 调用SocketChannelImpl的构造器
    return new SocketChannelImpl(this);
}
// sun.nio.ch.SocketChannelImpl
SocketChannelImpl(SelectorProvider sp) throws IOException {
    super(sp);
    // 创建socket fd
    this.fd = Net.socket(true);
    // 获取socket fd的值
    this.fdVal = IOUtil.fdVal(fd);
    // 初始化SocketChannel状态, 状态不多，总共就6个
    // 未初始化，未连接，正在连接，已连接，断开连接中，已断开
    this.state = ST_UNCONNECTED;
}
// sun.nio.ch.Net
static FileDescriptor socket(ProtocolFamily family, boolean stream)
    throws IOException {
    boolean preferIPv6 = isIPv6Available() &&
        (family != StandardProtocolFamily.INET);
    // 最后调用的是socket0
    return IOUtil.newFD(socket0(preferIPv6, stream, false));
}
// Due to oddities SO_REUSEADDR on windows reuse is ignored
private static native int socket0(boolean preferIPv6, boolean stream, boolean reuse);
```

底层还是socket函数

### 2、如何支持非阻塞
正常在c里我们实现非阻塞是靠fcntl这个函数，这个函数全称就是file control   
通过它可以管理fd的各种属性，比如设置fd的阻塞与否。

**fcntl的函数签名为:**
```
#include <fcntl.h>
int fcntl(int fildes, int cmd, ...);

```

第一个参数是传入的fd, 第二个参数是操作类型，后面是flag
要设置非阻塞，操作类型是F_SETFL和F_GETFL,flag是O_NONBLOCK

那么JVM是怎么做的呢，在SocketChannel上有一个configureBlocking函数，这个函数是设置当前SocketChannel是否是阻塞的，和selector一起用的时候一定要设置成非阻塞才有意义, 阻塞的话就不需要IO多路复用的事件通知了。



